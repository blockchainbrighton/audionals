<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Web3 Audio Sequencer Playback Engine Test</title>
</head>
<body>
    <h1>Web3 Audio Sequencer Playback Engine Test</h1>
    <p>Check the console for the 2D array of songs and channels with metadata.</p>

    
<!-- Section 1 - Load Song Files and Create 2D Channel Array with All Metadata -->
<SectionOne>

    <script>
        // ---------------------- Global Data ---------------------- //
        window.globalData = window.globalData || {
            isSingleSong: false,
            isMultipleSongs: false,
            songsArray: [],
            audioBuffers: {},
            reverseAudioBuffers: {},
            audioFetchCache: new Map() // Ensure audioFetchCache is initialized
        };

        // ---------------------- Configuration ---------------------- //

        // Key mapping for deserialization process
        const keyMap = {
            0: "projectName",
            1: "artistName",
            2: "projectBPM",
            3: "currentSequence",
            4: "channelURLs",
            5: "channelVolume",
            6: "channelPlaybackSpeed",
            7: "trimSettings",
            8: "projectChannelNames",
            9: "startSliderValue",
            10: "endSliderValue",
            11: "totalSampleDuration",
            12: "start",
            13: "end",
            14: "projectSequences",
            15: "steps"
        };

        // Reverse keyMap for reverse lookup
        const reverseKeyMap = Object.fromEntries(Object.entries(keyMap).map(([k, v]) => [v, +k]));

        // Map of letters 'A' to 'Z' representing channels and their reverse
        const channelMap = Array.from({ length: 26 }, (_, i) => String.fromCharCode(65 + i));
        const reverseChannelMap = Object.fromEntries(channelMap.map((letter, i) => [letter, i]));

        // ---------------------- Utility Functions ---------------------- //

        /** Dynamically import the Pako library from the specified path. */
        const loadPako = async () => {
            try {
                const pakoModule = await import('/content/fba6f95fb1152db43304a27dce8cb8c65509eba6ab0b6958cedeb33e5f443077i0');
                window.pako = pakoModule.default || pakoModule;
            } catch (error) {
                console.error("Error loading Pako library:", error);
                throw error;
            }
        };

        /** Decompress the steps data. */
        const decompressSteps = (steps) => steps.flatMap(step => {
            if (typeof step === "number") return step;
            if (step?.r) {
                const [start, end] = step.r;
                return Array.from({ length: end - start + 1 }, (_, i) => start + i);
            }
            if (typeof step === "string" && step.endsWith("r")) {
                return { index: parseInt(step.slice(0, -1), 10), reverse: true };
            }
            return [];
        });

        /** Deserialize the fetched JSON data using the provided keyMap. */
        const deserialize = (data) => {
            const recursiveDeserialize = (obj) => {
                if (Array.isArray(obj)) return obj.map(recursiveDeserialize);
                if (obj && typeof obj === "object") {
                    return Object.entries(obj).reduce((acc, [key, value]) => {
                        const mappedKey = keyMap[key] || key;
                        if (mappedKey === "projectSequences") {
                            acc[mappedKey] = Object.entries(value).reduce((seqAcc, [seqKey, seqValue]) => {
                                const sequenceName = `Sequence${seqKey.replace(/^s/, "")}`;
                                seqAcc[sequenceName] = Object.entries(seqValue).reduce((trackAcc, [trackKey, trackValue]) => {
                                    const channelName = `ch${reverseChannelMap[trackKey]}`;
                                    const steps = trackValue[reverseKeyMap.steps] || [];
                                    trackAcc[channelName] = { steps: decompressSteps(steps) };
                                    return trackAcc;
                                }, {});
                                return seqAcc;
                            }, {});
                        } else {
                            acc[mappedKey] = recursiveDeserialize(value);
                        }
                        return acc;
                    }, {});
                }
                return obj;
            };
            return recursiveDeserialize(data);
        };

        /** Fetch and deserialize data from a given URL. */
        const fetchAndDeserialize = async (url) => {
            try {
                const response = await fetch(url);
                if (!response.ok) throw new Error(`Network response was not ok for URL: ${url}`);
                const arrayBuffer = await response.arrayBuffer();
                const inflatedData = window.pako.inflate(new Uint8Array(arrayBuffer));
                const jsonString = new TextDecoder("utf-8").decode(inflatedData);
                return deserialize(JSON.parse(jsonString));
            } catch (error) {
                console.error(`Error fetching/deserializing URL ${url}:`, error);
                throw error;
            }
        };

        /** Fetch and process multiple URLs concurrently. */
        const fetchAndProcessData = async (urls) => {
            const fetchPromises = urls.map(url => fetchAndDeserialize(url).catch(error => {
                console.error(`Failed to process URL ${url}:`, error);
                return null;
            }));
            const results = (await Promise.all(fetchPromises)).filter(Boolean);
            if (!results.length) throw new Error("No valid data was processed.");
            return results;
        };

        /** Process the deserialized data to extract songs and their channels with metadata. */
        const processSongsAndChannels = (deserializedData) => {
            const songsArray = deserializedData.map((songData, songIndex) => {
                const {
                    projectName = `Song_${songIndex + 1}`,
                    projectSequences = {},
                    channelURLs = [],
                    channelVolume = [],
                    channelPlaybackSpeed = [],
                    trimSettings = {}
                } = songData;

                // Initialize channels
                const channels = Array.from({ length: 16 }, (_, i) => ({
                    id: channelMap[i] || `Channel_${i + 1}`,
                    url: channelURLs[i] || `URL_not_found`,
                    metadata: {
                        volume: channelVolume[i] ?? 1.0,
                        playbackSpeed: channelPlaybackSpeed[i] ?? 1.0,
                        trimStartTime_Percentage: trimSettings[i]?.start || 0,
                        trimEndTime_Percentage: trimSettings[i]?.end || 100,
                        requiresReversal: false // Initialize the flag
                    }
                }));

                // Analyze steps to determine if reversal is needed for each channel
                Object.values(projectSequences).forEach(sequence => {
                    Object.entries(sequence).forEach(([channelSequenceId, channelSequence]) => {
                        const steps = channelSequence.steps || [];

                        // Determine if any step requires reversal
                        const hasReverseStep = steps.some(step => {
                            if (typeof step === 'object' && step.reverse) return true;
                            // Add additional conditions if needed
                            return false;
                        });

                        if (hasReverseStep) {
                            // Extract numerical index from channelSequenceId (e.g., 'ch6' -> 6)
                            const match = channelSequenceId.match(/^ch(\d+)$/);
                            if (match) {
                                const channelIndex = parseInt(match[1], 10);
                                if (!isNaN(channelIndex) && channelIndex >= 0 && channelIndex < channels.length) {
                                    channels[channelIndex].metadata.requiresReversal = true;
                                } else {
                                    console.warn(`Invalid channel index extracted from ${channelSequenceId}`);
                                }
                            } else {
                                console.warn(`Invalid channelSequenceId format: ${channelSequenceId}`);
                            }
                        }
                    });
                });

                return {
                    id: projectName,
                    totalSequences: Object.keys(projectSequences).length,
                    channels,
                    projectSequences
                };
            });

            window.globalData.songsArray = songsArray;
            return songsArray;
        };

        /** Log the structured 2D array of songs and channels with metadata. */
        const logSongsArray = (songsArray) => {
            console.log(`Total Songs: ${songsArray.length}`);
            songsArray.forEach((song, songIndex) => {
                console.log(`\nSong #${songIndex + 1}: ${song.id} - Total Sequences: ${song.totalSequences}`);
                song.channels.forEach((channel, channelIndex) => {
                    const { volume, playbackSpeed, trimStartTime_Percentage, trimEndTime_Percentage, requiresReversal } = channel.metadata;
                    console.log(
                        `\tChannel ${channelIndex + 1} - ${channel.id}, ` +
                        `Volume: ${volume}, Playback Speed: ${playbackSpeed}, ` +
                        `Trim Start Time: ${trimStartTime_Percentage}, Trim End Time: ${trimEndTime_Percentage}, ` +
                        `Requires Reversal: ${requiresReversal}`
                    );
                });
                const projectSequencesJSON = JSON.stringify(song.projectSequences, null, 2);
                console.log(`\tProject Sequences for ${song.id}:\n${projectSequencesJSON}`);
            });

            window.globalData.isSingleSong = songsArray.length === 1;
            window.globalData.isMultipleSongs = songsArray.length > 1;
            console.log(`\nFlags set - isSingleSong: ${window.globalData.isSingleSong}, isMultipleSongs: ${window.globalData.isMultipleSongs}`);

            // Dispatch a custom event to notify that data loading is complete
            document.dispatchEvent(new CustomEvent("dataLoadingComplete", {
                detail: {
                    success: true,
                    totalSongs: songsArray.length,
                    songs: songsArray.map(({ id, totalSequences }) => ({ id, totalSequences }))
                }
            }));
        };

        // ---------------------- Initialization ---------------------- //

        /** Initialize the data processing workflow. */
        (async () => {
            const songDataUrls = [
                    // "/content/5527d0cc95ce5ce6eedf4e275234da8b1fe087512d0db618b6de1aaad437c96bi0", // TRUTH
                    // "/content/119a3ccd1dfd7e987cca139f86d16717d845a22dd6afc59ad492527b95ae9a91i0", // MLK I HAVE A DREAM
                    "/content/6d288c0c82653001bb32497889dd1486e8afec9b0671a95fa9e10f99c20737bbi0", // KORA
                    // "/content/8aec0a99a5617b9da98a5b63a11a5143f0cac3cfa662d9515c2285de03ef95d4i0", // CHEESE ** MIGHT BE THIS ONE THAT IS OUT OF SYNC??
                    // "/content/db9131cfe8e933e8e639f007dcd2b582a80bfd2be42b0eafa4d2e206332d6785i0", // ModernProgress
                    // "/content/07ff7bdc47e5272a3ff55cc46d2b189d510562a057a2c24112f3d0376950484di0", // CHOPPIN' IT UP
                    // "/content/fb0d2abcd1fa5bf2622579f0990435b48d41291f71626fc2e36a93e6ea6b3b85i0", // HUMANITY
                    // "/content/3359ce42359274ddbd2184d9f75a38b7e59b1d5f24512959e29c377fc8ca604ai0", // MintyFresh Vibes
                    // "/content/633100d631767ddb9a309f5a2a66f5a66d5abd839f3b1c55642690d484189971i0", // ON DAY ONE
                    // "/content/85436950f53c57aa0c510071d2d5f1c187e1d21e4e57210fcae152c4c7b6a768i0", // Rhythm and Bass 240
                    // "/content/e3ca12dd7516b4e486af4e3fa7f4ebc535d825034ff3c9da4954f354572dcf61i0", // Crazy Ass Bitch
                    // "/content/d0496a8e1657ce470807c8d47dcb5f1018a32d8ec8e50d490ad49411ffee1457i0", // Rhythm and Bass 60
                    // "/content/b22f1c85371b58a9cdac19b2baa50b1f9025a28d44cdfaad539d0527aa7d894ei0", // ON-CHAIN IN THE MEMBRANE
                // Add additional URLs as needed
            ];

            const validSongUrls = songDataUrls.filter(url => url.trim() && !url.trim().startsWith('//'));
            if (validSongUrls.length) {
                try {
                    await loadPako();
                    const deserializedData = await fetchAndProcessData(validSongUrls);
                    const songsArray = processSongsAndChannels(deserializedData);
                    logSongsArray(songsArray);
                } catch (error) {
                    console.error('Error during initialization:', error);
                }
            } else {
                console.log('No valid song data URLs to process.');
            }
        })();
    </script>

    <!-- By the end of Section 1, all song data is arranged in a 2D array of channels along with their metadata -->
</SectionOne>


<!-- Section 2 - Audio Buffering and Mapping -->
<sectionTwo>

    <script>
        // ---------------------- Audio Buffering and Mapping ---------------------- //

        // Ensure that globalData exists
        window.globalData = window.globalData || {};

        // Initialize each property individually to avoid overwriting existing data
        window.globalData.songsArray = window.globalData.songsArray || [];
        window.globalData.audioBuffers = window.globalData.audioBuffers || {};
        window.globalData.reverseAudioBuffers = window.globalData.reverseAudioBuffers || {};
        window.globalData.audioFetchCache = window.globalData.audioFetchCache || new Map();

        // Initialize or reuse the AudioContext
        const audioContext = window.globalData.audioContext || new (window.AudioContext || window.webkitAudioContext)();
        window.globalData.audioContext = audioContext;

        /**
         * Converts a base64 string to an ArrayBuffer.
         * @param {string} base64 - The base64 encoded string.
         * @returns {ArrayBuffer} - The resulting ArrayBuffer.
         */
        const base64ToArrayBuffer = (base64) => {
            const binaryString = atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        };

        /**
         * Extract base64 encoded audio data from an HTML response.
         * @param {string} html - The HTML content.
         * @returns {string|null} - The extracted base64 string or null if not found.
         */
        const extractBase64FromHTML = (html) => {
            const audioSrcMatch = html.match(/<audio[^>]*data-audionalSampleName[^>]*>\s*<source[^>]*src="([^"]+)"/i);
            if (audioSrcMatch && audioSrcMatch[1].includes("base64")) {
                const src = audioSrcMatch[1];
                const base64Index = src.indexOf("base64,");
                if (base64Index !== -1) {
                    return src.substring(base64Index + 7); // Extract string after 'base64,'
                }
            }
            return null;
        };

        /**
         * Extract base64 encoded audio data from a JSON response.
         * @param {string} jsonData - The JSON string.
         * @returns {string|null} - The extracted base64 string or null if not found.
         */
        const extractBase64FromJSON = (jsonData) => {
            try {
                const parsed = JSON.parse(jsonData);
                if (parsed.audioData) {
                    const dataUri = parsed.audioData;
                    const base64Index = dataUri.indexOf("base64,");
                    if (base64Index !== -1) {
                        return dataUri.substring(base64Index + 7); // Extract string after 'base64,'
                    }
                }
            } catch (e) {
                console.warn("Failed to parse JSON data for base64 extraction:", e);
            }
            return null;
        };

        /**
         * Validates whether a given string is a valid base64 encoded string.
         * @param {string} str - The string to validate.
         * @returns {boolean} - Returns true if valid base64, else false.
         */
        const isValidBase64 = (str) => {
            try {
                // Remove any whitespace or newlines
                const cleanedStr = str.replace(/\s+/g, '');
                // Check if length is a multiple of 4
                if (cleanedStr.length % 4 !== 0) return false;
                // Attempt to decode
                atob(cleanedStr);
                return true;
            } catch (e) {
                return false;
            }
        };

        /**
         * Fetch and decode audio data with caching and validation.
         * @param {Response} response - The fetch response object.
         * @param {string} contentType - The content type of the response.
         * @param {string} url - The URL of the fetched resource.
         * @returns {Promise<AudioBuffer>} - Decoded audio buffer.
         */
        const fetchAndDecodeAudio = async (response, contentType, url) => {
            const cache = window.globalData.audioFetchCache;

            // Return cached buffer if available
            if (cache.has(url)) {
                return cache.get(url);
            }

            let audioBuffer;

            try {
                if (/audio\/(wav|mpeg|mp4)|video\/mp4/.test(contentType)) {
                    // Directly decode supported audio formats
                    const arrayBuffer = await response.arrayBuffer();
                    audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                } else if (/application\/json/.test(contentType)) {
                    // Handle JSON files containing base64 encoded audioData
                    const textData = await response.text();
                    const base64Data = extractBase64FromJSON(textData);
                    if (base64Data && isValidBase64(base64Data)) {
                        const arrayBuffer = base64ToArrayBuffer(base64Data);
                        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                    } else {
                        throw new Error("Invalid or missing base64 audio data in JSON.");
                    }
                } else if (/text\/html/.test(contentType)) {
                    // Handle HTML files containing base64 encoded audioData
                    const textData = await response.text();
                    const base64Data = extractBase64FromHTML(textData);
                    if (base64Data && isValidBase64(base64Data)) {
                        const arrayBuffer = base64ToArrayBuffer(base64Data);
                        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                    } else {
                        throw new Error("Invalid or missing base64 audio data in HTML.");
                    }
                } else if (/audio\//.test(contentType)) {
                    // Handle other audio content types
                    const arrayBuffer = await response.arrayBuffer();
                    audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                } else {
                    throw new Error("Unsupported content type or missing audio data.");
                }

                // Cache the decoded audio buffer
                cache.set(url, audioBuffer);
                return audioBuffer;
            } catch (error) {
                console.error(`Error decoding audio from URL ${url}:`, error);
                throw error; // Rethrow to be caught in processChannel
            }
        };

        /**
         * Optimized reverse function for Float32Array.
         * @param {Float32Array} array - The audio data array to reverse.
         * @returns {Float32Array} - The reversed audio data array.
         */
        const reverseArray = (array) => {
            const len = array.length;
            const reversed = new Float32Array(len);
            for (let i = 0; i < len; i++) {
                reversed[i] = array[len - i - 1];
            }
            return reversed;
        };

        /**
         * Create a reversed AudioBuffer from an existing AudioBuffer.
         * Only called if the channel requires reversal.
         * @param {AudioBuffer} audioBuffer - The audio buffer to reverse.
         * @returns {AudioBuffer} - The reversed audio buffer.
         */
        const createReverseAudioBuffer = (audioBuffer) => {
            const reverseBuffer = audioContext.createBuffer(
                audioBuffer.numberOfChannels,
                audioBuffer.length,
                audioBuffer.sampleRate
            );

            for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
                const originalData = audioBuffer.getChannelData(channel);
                const reversedData = reverseBuffer.getChannelData(channel);
                reversedData.set(reverseArray(originalData));
            }

            return reverseBuffer;
        };

        /**
         * Extract file name from URL.
         * Handles edge cases gracefully.
         * @param {string} url - The URL of the audio file.
         * @returns {string} - The extracted file name or "Unknown".
         */
        const extractFileName = (url) => {
            try {
                const urlParts = url.split('/');
                return urlParts[urlParts.length - 1] || "Unknown";
            } catch (e) {
                console.warn(`Could not extract file name from URL: ${url}`);
                return "Unknown";
            }
        };

        /**
         * Process a single channel:
         * - Fetch and decode audio
         * - Trim audio based on metadata
         * - Conditionally create a reversed buffer if required
         * - Cache the buffers
         * - Log processing information
         * @param {Object} song - The song object.
         * @param {Object} channel - The channel object.
         * @param {Array} logEntries - Array to log processing information.
         */
        const processChannel = async (song, channel, logEntries) => {
            try {
                const response = await fetch(channel.url);
                if (!response.ok) throw new Error(`Network response was not ok for URL: ${channel.url}`);

                // Decode audio with caching
                const audioBuffer = await fetchAndDecodeAudio(response, response.headers.get("Content-Type"), channel.url);

                // Get trimming information
                const { trimStartTime_Percentage, trimEndTime_Percentage, requiresReversal } = channel.metadata;

                if (trimEndTime_Percentage <= trimStartTime_Percentage) {
                    console.warn(`Trim end percentage is less than or equal to start percentage for Song: ${song.id}, Channel: ${channel.id}`);
                    return;
                }

                const trimStartTime = (trimStartTime_Percentage / 100) * audioBuffer.duration;
                const trimEndTime = (trimEndTime_Percentage / 100) * audioBuffer.duration;

                const startSample = Math.floor(trimStartTime * audioBuffer.sampleRate);
                const endSample = Math.floor(trimEndTime * audioBuffer.sampleRate);

                const totalSamples = audioBuffer.length;
                const clampedStartSample = Math.min(Math.max(startSample, 0), totalSamples - 1);
                const clampedEndSample = Math.min(Math.max(endSample, 0), totalSamples);

                const trimmedLength = clampedEndSample - clampedStartSample;

                if (trimmedLength <= 0) {
                    console.warn(`Trimmed length is non-positive for Song: ${song.id}, Channel: ${channel.id}`);
                    return;
                }

                // Create trimmed AudioBuffer
                const trimmedAudioBuffer = audioContext.createBuffer(
                    audioBuffer.numberOfChannels,
                    trimmedLength,
                    audioBuffer.sampleRate
                );

                for (let channelIndex = 0; channelIndex < audioBuffer.numberOfChannels; channelIndex++) {
                    const originalChannelData = audioBuffer.getChannelData(channelIndex);
                    const trimmedChannelData = trimmedAudioBuffer.getChannelData(channelIndex);
                    trimmedChannelData.set(
                        originalChannelData.subarray(clampedStartSample, clampedEndSample)
                    );
                }

                // Initialize storage for the song if not already present
                window.globalData.audioBuffers[song.id] = window.globalData.audioBuffers[song.id] || {};
                window.globalData.reverseAudioBuffers[song.id] = window.globalData.reverseAudioBuffers[song.id] || {};

                // Cache the trimmed buffer
                window.globalData.audioBuffers[song.id][channel.id] = trimmedAudioBuffer;

                // Conditionally create and cache the reversed buffer
                if (requiresReversal) {
                    const reversedTrimmedAudioBuffer = createReverseAudioBuffer(trimmedAudioBuffer);
                    window.globalData.reverseAudioBuffers[song.id][channel.id] = reversedTrimmedAudioBuffer;
                }

                const audioFileName = extractFileName(channel.url);
                const durationAfterTrimming = trimmedAudioBuffer.duration.toFixed(2);

                // Log processing information
                logEntries.push({
                    songId: song.id,
                    channelId: channel.id,
                    audioFileName: audioFileName,
                    fullDuration: audioBuffer.duration.toFixed(2),
                    durationAfterTrimming: durationAfterTrimming,
                    requiresReversal: requiresReversal
                });

                console.log(`Processed audio for Song: ${song.id}, Channel: ${channel.id}${requiresReversal ? " (Reversed)" : ""}`);
            } catch (error) {
                console.error(`Failed to process audio for Song: ${song.id}, Channel: ${channel.id}`, error);
                // Optionally, you can push an error entry to logEntries if needed
            }
        };

        /**
         * Logs detailed information about each processed channel.
         * @param {Array} logEntries - The array of log entries.
         */
        const logDetailedInfo = (logEntries) => {
            if (logEntries.length > 0) {
                console.log("----- Audio Samples Duration Information -----");
                console.table(logEntries.map(entry => ({
                    "Song ID": entry.songId,
                    "Channel ID": entry.channelId,
                    "Audio File Name": entry.audioFileName,
                    "Full Duration (s)": entry.fullDuration,
                    "Duration After Trimming (s)": entry.durationAfterTrimming,
                    "Requires Reversal": entry.requiresReversal
                })));
                console.log("----- End of Audio Samples Duration Information -----");
            } else {
                console.warn("No audio samples were processed successfully.");
            }
        };

        /**
         * Process all audio channels in parallel with optimized asynchronous handling.
         */
        const processAllAudioChannels = async () => {
            const { songsArray } = window.globalData;
            if (!songsArray.length) {
                console.error("No songs available to process.");
                return;
            }

            const logEntries = [];

            // Prepare all channel processing promises
            const allChannelPromises = songsArray.flatMap(song => {
                return song.channels.map(channel => processChannel(song, channel, logEntries));
            });

            // Execute all channel processing in parallel using Promise.allSettled to handle all promises
            const results = await Promise.allSettled(allChannelPromises);

            // Handle any rejected promises
            results.forEach((result, index) => {
                if (result.status === 'rejected') {
                    console.warn(`Processing failed for Channel ${index + 1}:`, result.reason);
                }
            });

            // Log detailed information
            logDetailedInfo(logEntries);

            console.log("All trimmed audio buffers and reverse audio buffers have been created and mapped.");
            document.dispatchEvent(new CustomEvent("audioBuffersReady", { detail: { success: true } }));
        };

        /**
         * Ensures that the AudioContext is running.
         * Resumes the context if it is suspended.
         */
        const ensureAudioContextRunning = async () => {
            if (audioContext.state === 'suspended') {
                await audioContext.resume();
            }
        };

        // ---------------------- Initialization for Section Two ---------------------- //

        /**
         * Initialize the audio processing workflow.
         * Ensures that AudioContext is active and processes all audio channels.
         */
        const initAudioProcessing = async () => {
            try {
                await ensureAudioContextRunning();
                await processAllAudioChannels();
            } catch (error) {
                console.error("Error during audio processing initialization:", error);
            }
        };

        // Listen for the completion of data loading and processing from Section One
        document.addEventListener("dataLoadingComplete", initAudioProcessing);

        // If Section One already processed data, initiate audio processing immediately
        if (window.globalData.songsArray.length) initAudioProcessing();
    </script>

    <!-- By the end of Section 2, all audio buffers and reverse audio buffers are created and mapped -->
</sectionTwo>


<!-- Section 3 - Playback Engine -->
<!-- This section handles playback of the sequenced audio based on sequences, channels, and steps -->
<sectionThree>

    <script>
        // ---------------------- Playback Engine Enhancements ---------------------- //

        // Configuration Constants
        const selectedBPM = 120; // Beats Per Minute
        const stepsPerBeat = 4; // Number of steps per beat
        let stepDuration = (60 / selectedBPM) / stepsPerBeat; // Duration of each step in seconds

        // Scheduler Configuration
        const lookahead = 0.1; // How far ahead to schedule audio (in seconds)
        const schedulerInterval = 25; // Scheduler loop interval (in milliseconds)

        // Scheduler Variables
        let isPlaying = false; // Playback state
        let schedulerTimerID = null; // Timer ID for the scheduler
        const sequenceStates = {}; // Holds the state for each sequence

        /**
         * Initialize the playback engine.
         * This function is called after audio buffers are ready and sets up necessary variables.
         */
        const initPlaybackEngine = () => {
            const { songsArray } = window.globalData;
            if (!songsArray.length) {
                console.error("No songs available for playback.");
                return;
            }

            console.log("Playback Engine Initialization Complete.");
            console.log("Playback is ready. Press 'p' to start.");
        };

        /**
         * Start the playback engine.
         * Initializes scheduling variables and starts the scheduler loop.
         */
        const startPlayback = () => {
            const { songsArray, audioBuffers, reverseAudioBuffers, audioContext } = window.globalData;
            if (!songsArray.length) {
                console.error("No songs available for playback.");
                return;
            }

            const song = songsArray[0]; // Assuming playback of the first song
            const sequences = song.projectSequences || {};
            console.log(`Starting playback for Song: ${song.id} with ${Object.keys(sequences).length} sequences.`);

            // Calculate total duration per sequence
            const sequenceDuration = 64 * stepDuration; // 64 steps per sequence

            // Initialize scheduling variables for each sequence
            let sequenceStartOffset = 0;
            for (const [sequenceName, sequence] of Object.entries(sequences)) {
                sequenceStates[sequenceName] = {
                    nextStepIndex: 0,
                    nextStepTime: audioContext.currentTime + sequenceStartOffset,
                };
                console.log(`Initialized scheduler for sequence: ${sequenceName} starting at +${sequenceStartOffset.toFixed(2)}s`);
                sequenceStartOffset += sequenceDuration; // Next sequence starts after current sequence
            }

            isPlaying = true;

            // Start the scheduler loop
            schedulerTimerID = setInterval(() => {
                schedulerLoop(song, audioContext, audioBuffers, reverseAudioBuffers);
            }, schedulerInterval);

            console.log('Playback started.');
        };

        /**
         * Stop the playback engine.
         * Clears the scheduler loop and resets playback variables.
         */
        const stopPlayback = () => {
            if (schedulerTimerID) {
                clearInterval(schedulerTimerID);
                schedulerTimerID = null;
            }
            isPlaying = false;

            // Reset sequence states
            for (const sequenceName in sequenceStates) {
                sequenceStates[sequenceName].nextStepIndex = 0;
                sequenceStates[sequenceName].nextStepTime = 0;
            }

            console.log('Playback stopped and sequence states reset.');
        };

        /**
         * Scheduler Loop Function.
         * Continuously schedules audio events within the look-ahead window.
         * @param {Object} song - The song object.
         * @param {AudioContext} audioContext - The AudioContext instance.
         * @param {Object} audioBuffers - The audio buffers object.
         * @param {Object} reverseAudioBuffers - The reversed audio buffers object.
         */
        const schedulerLoop = (song, audioContext, audioBuffers, reverseAudioBuffers) => {
            const currentTime = audioContext.currentTime;

            for (const [sequenceName, sequence] of Object.entries(song.projectSequences || {})) {
                const state = sequenceStates[sequenceName];
                if (!state) continue;

                // Schedule steps within the lookahead window
                while (state.nextStepTime < currentTime + lookahead && isPlaying) {
                    const stepIndex = state.nextStepIndex;
                    const stepTime = state.nextStepTime;

                    // Iterate over each channel (track) in the sequence
                    for (const [trackName, trackData] of Object.entries(sequence)) {
                        const channelIndex = parseInt(trackName.replace('ch', ''), 10);
                        const channel = song.channels[channelIndex];

                        if (!channel) {
                            console.warn(`Channel ${trackName} not found in song ${song.id}.`);
                            continue;
                        }

                        const steps = trackData.steps || [];

                        // Check if this stepIndex is in the steps array
                        const step = steps.find(s => {
                            if (typeof s === "number") {
                                return s === stepIndex;
                            } else if (typeof s === "object" && s.index !== undefined) {
                                return s.index === stepIndex;
                            }
                            return false;
                        });

                        if (step !== undefined) {
                            const reverse = (typeof step === "object" && step.reverse) || false;
                            schedulePlayback(song, channel, stepTime, reverse, audioBuffers, reverseAudioBuffers);
                        }
                    }

                    // Increment for next step
                    state.nextStepIndex = (state.nextStepIndex + 1) % 64; // Reset after 64 steps
                    state.nextStepTime += stepDuration;
                }
            }
        };

        /**
         * Schedule playback for a single channel at a specific time.
         * @param {Object} song - The song object.
         * @param {Object} channel - The channel object.
         * @param {number} time - The time at which to play the step.
         * @param {boolean} reverse - Whether to play in reverse.
         * @param {Object} audioBuffers - The audio buffers object.
         * @param {Object} reverseAudioBuffers - The reversed audio buffers object.
         */
        const schedulePlayback = (song, channel, time, reverse = false, audioBuffers, reverseAudioBuffers) => {
            const buffer = reverse ? reverseAudioBuffers[song.id][channel.id] : audioBuffers[song.id][channel.id];

            if (!buffer) {
                console.error(`Audio buffer for Song: ${song.id}, Channel: ${channel.id} not found.`);
                return;
            }

            // Ensure buffer duration does not exceed stepDuration to prevent overlapping playback
            if (buffer.duration > stepDuration) {
                console.warn(`Buffer duration (${buffer.duration}s) for Channel: ${channel.id} exceeds stepDuration (${stepDuration}s). This may cause overlapping playback.`);
                // Optionally, trim the buffer or adjust stepDuration here
            }

            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.playbackRate.value = channel.metadata.playbackSpeed;
            source.connect(audioContext.destination);
            source.start(time);
            console.log(`    Scheduled ${reverse ? 'reverse ' : ''}playback for Channel: ${channel.id} at time ${time.toFixed(2)}s.`);

            // Optional: Handle buffer source completion
            source.onended = () => {
                console.log(`Playback ended for Channel: ${channel.id} at time ${audioContext.currentTime.toFixed(2)}s.`);
                // Perform any cleanup or trigger events if necessary
            };
        };

        /**
         * Adjust the BPM during playback.
         * @param {number} newBPM - The new BPM value.
         */
        const adjustBPM = (newBPM) => {
            if (newBPM <= 0) {
                console.warn("Invalid BPM value. Must be greater than 0.");
                return;
            }
            stepDuration = (60 / newBPM) / stepsPerBeat;
            console.log(`BPM adjusted to ${newBPM}. New step duration: ${stepDuration.toFixed(4)}s.`);
        };

        /**
         * Listener function for 'p' key to start/stop playback.
         * @param {KeyboardEvent} event - The keyboard event.
         */
        const onKeyPress = (event) => {
            if (event.key.toLowerCase() === 'p') {
                if (!isPlaying) {
                    startPlayback();
                    console.log('Playback started by user.');
                } else {
                    stopPlayback();
                    console.log('Playback stopped by user.');
                }
            }
        };

        // Initialize Playback Engine
        const initializePlayback = () => {
            console.log("Audio buffers are ready. Playback can begin.");
            initPlaybackEngine();
        };

        // Event Listeners
        document.addEventListener("audioBuffersReady", () => {
            initializePlayback();
            console.log('Press "p" to start playback.');
            document.addEventListener('keydown', onKeyPress);
        });

        // If already loaded
        if (Object.keys(window.globalData.audioBuffers).length) {
            initializePlayback();
            console.log('Press "p" to start playback.');
            document.addEventListener('keydown', onKeyPress);
        };

        /**
         * Structure Explanation:
         * - A Sequence contains 64 steps.
         * - Each Sequence can contain any number of channels.
         * - Each channel in a sequence has steps where audio is played.
         * - Steps can be either:
         *   - A number representing the step index (0 to 63).
         *   - An object with 'index' and 'reverse' properties for reverse playback.
         * - There are 4 steps per beat.
         * - Time per step is calculated based on BPM: stepDuration = (60 / BPM) / 4.
         * - Playback is scheduled step by step using a scheduler loop, ensuring synchronization with the step sequencer.
         * - Playback direction (normal or reverse) is determined by the 'reverse' property in steps.
         * - Press 'p' to start or stop playback.
         * - BPM can be adjusted dynamically using the adjustBPM function for DJ nudging.
         */
    </script>

    <!-- By the end of Section 3, playback should be initialized and ready -->
</sectionThree>

</body>
</html>
